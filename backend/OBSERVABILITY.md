# Observability & Monitoring

## Overview

The Enterprise AI Assistant Platform includes comprehensive observability through Opik integration, providing distributed tracing, metrics, and logging for all agent interactions.

## Tracing Architecture

### Instrumentation Points

```
User Request
    â”‚
    â”œâ”€â–º API Endpoint (traced)
    â”‚   â””â”€â–º Request validation
    â”‚
    â”œâ”€â–º Guardrail Check (traced)
    â”‚   â””â”€â–º Pattern matching
    â”‚
    â”œâ”€â–º Session Retrieval (traced)
    â”‚   â””â”€â–º Context building
    â”‚
    â”œâ”€â–º Agent Routing (traced)
    â”‚   â””â”€â–º Confidence scoring
    â”‚
    â”œâ”€â–º Agent Query (traced)
    â”‚   â”œâ”€â–º HR Agent
    â”‚   â”œâ”€â–º Analytics Agent
    â”‚   â”‚   â””â”€â–º MCP Tool Calls (traced)
    â”‚   â””â”€â–º Document Agent
    â”‚       â”œâ”€â–º Document Chunking (traced)
    â”‚       â”œâ”€â–º Embedding Generation (traced)
    â”‚       â””â”€â–º Similarity Search (traced)
    â”‚
    â””â”€â–º Response Assembly (traced)
        â””â”€â–º Session update
```

## Sample Trace

### Trace Metadata

```json
{
  "trace_id": "trace_abc123xyz456",
  "session_id": "sess_789def012",
  "user_id": "user_456",
  "timestamp": "2025-11-19T10:30:45.123Z",
  "duration_ms": 2341,
  "status": "success",
  "agent_used": "hr",
  "model_provider": "bedrock",
  "model_id": "us.anthropic.claude-sonnet-4-5-20250929-v1:0"
}
```

### Trace Timeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Trace: HR Agent Query                                       â”‚
â”‚ Duration: 2.34s                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

0ms     â”œâ”€â–º API Request Received
        â”‚   Input: "Who reports to Jennifer Lee?"
        â”‚
50ms    â”œâ”€â–º Guardrail Check
        â”‚   Status: PASSED
        â”‚   Duration: 50ms
        â”‚
120ms   â”œâ”€â–º Session Retrieval
        â”‚   Session: sess_789def012
        â”‚   History: 2 messages
        â”‚   Duration: 70ms
        â”‚
250ms   â”œâ”€â–º Agent Routing
        â”‚   Decision: hr_agent
        â”‚   Confidence: 0.95
        â”‚   Reasoning: "Organizational structure query"
        â”‚   Duration: 130ms
        â”‚
2100ms  â”œâ”€â–º HR Agent Query
        â”‚   â”œâ”€â–º LLM Call (Bedrock)
        â”‚   â”‚   Model: claude-sonnet-4-5
        â”‚   â”‚   Tokens: 156 input, 89 output
        â”‚   â”‚   Duration: 1850ms
        â”‚   â”‚
        â”‚   â””â”€â–º Response: "John Smith (Senior Software Engineer)..."
        â”‚
2341ms  â””â”€â–º Response Sent
            Status: 200 OK
            Total Duration: 2.34s
```

## Metrics Dashboard

### Key Performance Indicators

#### Request Metrics
```
Total Requests (24h):        1,247
Success Rate:                98.4%
Average Latency:             1.8s
P95 Latency:                 3.2s
P99 Latency:                 5.1s
Error Rate:                  1.6%
```

#### Agent Distribution
```
HR Agent:                    42% (524 requests)
Analytics Agent:             31% (387 requests)
Document Agent:              19% (237 requests)
Root (General):              8% (99 requests)
```

#### Token Usage (Cost Tracking)
```
Total Tokens (24h):          1,245,678
Input Tokens:                856,234
Output Tokens:               389,444
Estimated Cost:              $18.45
Average Tokens/Request:      998
```

#### Guardrail Triggers
```
Total Checks:                1,247
Blocked Requests:            23 (1.8%)
Prompt Injection:            15
PII Exposure:                5
Malicious Content:           3
```

## Sample Logs

### Structured Log Format

```json
{
  "timestamp": "2025-11-19T10:30:45.123Z",
  "level": "INFO",
  "service": "enterprise-ai-backend",
  "component": "hr_agent",
  "trace_id": "trace_abc123xyz456",
  "session_id": "sess_789def012",
  "message": "Processing HR query",
  "metadata": {
    "agent": "hr",
    "confidence": 0.95,
    "model": "claude-sonnet-4-5",
    "latency_ms": 1850,
    "tokens_input": 156,
    "tokens_output": 89
  }
}
```

### Log Levels

#### INFO - Normal Operations
```
2025-11-19 10:30:45 INFO [root_chatbot] Processing message for session sess_789def012
2025-11-19 10:30:45 INFO [agent_router] Routing to hr_agent (confidence: 0.95)
2025-11-19 10:30:47 INFO [hr_agent] Successfully received response from hr_agent
```

#### WARNING - Recoverable Issues
```
2025-11-19 10:31:12 WARNING [document_agent] Qdrant connection failed. Using fallback mode.
2025-11-19 10:31:15 WARNING [bedrock_integration] Rate limit hit. Retrying in 2s...
```

#### ERROR - Failures
```
2025-11-19 10:32:01 ERROR [analytics_agent] Exception querying agent: Connection timeout
2025-11-19 10:32:01 ERROR [root_chatbot] All agents failed. Handling with root chatbot.
```

## Opik Dashboard

### Accessing Traces

1. **Cloud Dashboard**: https://www.comet.com/opik
2. **Login**: Use credentials from `.env` file
3. **Workspace**: Your configured workspace name

### Dashboard Views

#### Traces View
- List of all traces with filters
- Search by session_id, user_id, agent
- Sort by duration, timestamp, status

#### Sessions View
- Conversation history per session
- Multi-turn interaction tracking
- User journey analysis

#### Metrics View
- Request volume over time
- Latency percentiles
- Error rate trends
- Token usage and costs

#### Agents View
- Performance by agent type
- Success rates
- Average latency
- Token efficiency

## Alerting

### Alert Conditions

#### Critical Alerts ðŸ”´
```
- Error rate > 5% (5 min window)
- P95 latency > 10s (5 min window)
- Guardrail block rate > 10% (15 min window)
- Service unavailable (health check fails)
```

#### Warning Alerts ðŸŸ¡
```
- Error rate > 2% (15 min window)
- P95 latency > 5s (15 min window)
- Token usage > $100/day
- Qdrant connection failures
```

### Alert Channels
- Email: ops-team@company.com
- Slack: #ai-platform-alerts
- PagerDuty: Critical alerts only

## Performance Optimization

### Identified Bottlenecks

1. **LLM Latency** (1.8s avg)
   - Optimization: Prompt caching, smaller models for simple queries
   - Target: < 1.5s

2. **Vector Search** (500ms avg)
   - Optimization: Index tuning, reduce k value
   - Target: < 300ms

3. **Session Retrieval** (70ms avg)
   - Optimization: In-memory caching, Redis
   - Target: < 20ms

### Optimization Results

```
Before Optimization:
- Average Latency: 2.8s
- P95 Latency: 5.2s
- Token Usage: 1,200 tokens/request

After Optimization:
- Average Latency: 1.8s (-36%)
- P95 Latency: 3.2s (-38%)
- Token Usage: 998 tokens/request (-17%)
```

## Debugging with Traces

### Example: Investigating Slow Request

**Problem**: User reports slow response for document query

**Step 1**: Find trace by session_id
```
Session: sess_abc123
Trace: trace_xyz789
Duration: 8.2s (slow!)
```

**Step 2**: Analyze trace timeline
```
0ms     API Request
50ms    Guardrail Check (normal)
120ms   Session Retrieval (normal)
250ms   Agent Routing (normal)
8100ms  Document Agent Query (SLOW!)
  â”œâ”€ 200ms  Similarity Search (normal)
  â”œâ”€ 7800ms LLM Call (SLOW!)
  â””â”€ 100ms  Response Assembly
```

**Step 3**: Identify root cause
- LLM call took 7.8s (expected: 2s)
- Large context window (3,500 tokens)
- Bedrock throttling detected

**Step 4**: Apply fix
- Reduce context window to 2,000 tokens
- Implement sliding window for history
- Add retry logic with backoff

**Result**: Latency reduced to 2.1s âœ…

## Cost Tracking

### Token Usage by Agent

```
HR Agent:
- Avg Input Tokens: 180
- Avg Output Tokens: 95
- Cost per Request: $0.012

Analytics Agent:
- Avg Input Tokens: 220
- Avg Output Tokens: 150
- Cost per Request: $0.018

Document Agent:
- Avg Input Tokens: 850 (includes context)
- Avg Output Tokens: 200
- Cost per Request: $0.045
```

### Daily Cost Breakdown

```
Total Daily Cost: $18.45

By Agent:
- HR Agent:        $6.29 (34%)
- Analytics Agent: $6.97 (38%)
- Document Agent:  $5.19 (28%)

By Operation:
- LLM Inference:   $16.20 (88%)
- Embeddings:      $1.85 (10%)
- Vector Storage:  $0.40 (2%)
```

## Best Practices

### Tracing
âœ… Trace all agent interactions
âœ… Include relevant metadata (model, tokens, confidence)
âœ… Use consistent naming conventions
âœ… Add tags for filtering (agent:hr, operation:query)

### Logging
âœ… Use structured logging (JSON)
âœ… Never log PII or credentials
âœ… Include trace_id for correlation
âœ… Use appropriate log levels

### Metrics
âœ… Track latency percentiles (p50, p95, p99)
âœ… Monitor error rates by endpoint
âœ… Track token usage for cost control
âœ… Set up alerts for anomalies

### Cost Optimization
âœ… Monitor token usage trends
âœ… Optimize prompts for efficiency
âœ… Use caching where possible
âœ… Consider smaller models for simple queries

---

**Last Updated**: 2025-11-19
**Version**: 1.0.0
